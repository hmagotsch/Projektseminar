# -*- coding: utf-8 -*-
"""Ohne LocIDS von Fine_Tuning von XGBoost.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/11Gt5TUsvyCuWBr5A-Va8fLzv_4W71hi7
"""

import tensorflow as tf
import os
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import imblearn
import xgboost as xgb
from sklearn.model_selection import train_test_split
from xgboost import plot_importance, plot_tree
from sklearn.metrics import mean_squared_error, mean_absolute_error
import seaborn as sns
from sklearn.preprocessing import LabelEncoder
from xgboost import XGBClassifier
import xgboost as xgb
from sklearn.metrics import mean_squared_error
color_pal = sns.color_palette()
from sklearn.metrics import accuracy_score, classification_report

path="/content/drive/MyDrive/MachineB_with_Job.csv"
df=pd.read_csv(path,sep=";")
df.head(20)
df.info()

df=df[['Gross','MsgRank','Speed','CheckIn', 'Net','SeqNr','Job']]
df.info()

#convert checkIn to datetime|old version
#df['CheckIn']=pd.to_datetime(df['CheckIn'],format='%Y-%m-%d %H:%M:%S.%f')

#convert CheckIn and CheckOut to datetime
date_format = "%Y-%m-%d %H:%M:%S%z"
df['CheckIn'] = df['CheckIn'].str.replace('\.\d+', '', regex=True)
df['CheckIn'] = pd.to_datetime(df['CheckIn'], format=date_format)

#Extracting features from CheckIn
df['CheckInHour'] = df['CheckIn'].dt.hour
df['CheckInMinute'] = df['CheckIn'].dt.minute
df['CheckInSecond'] = df['CheckIn'].dt.second
df['CheckInMicrosecond'] = df['CheckIn'].dt.microsecond
df['CheckInYear'] = df['CheckIn'].dt.year
df['CheckInMonth'] = df['CheckIn'].dt.month
df['CheckInDay'] = df['CheckIn'].dt.day

df = df.drop(df.index[df['MsgRank']<0])

le = LabelEncoder()
df['MsgRank_encoded']= le.fit_transform(df['MsgRank'])

bye=['MsgRank_encoded','MsgRank','CheckIn']
X=df.drop(bye,axis=1)
y=df['MsgRank_encoded']

#train test
X_train,X_test,y_train,y_test=train_test_split(X, y,test_size=0.3,shuffle=False,random_state=80)

"""Create XGBoost Model"""

from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, roc_curve

model1=XGBClassifier(random_state=42)
model1.fit(X_train, y_train)#


feature_importances = model1.feature_importances_

# Create a DataFrame for better visualization
importance_df = pd.DataFrame({'Feature': X.columns, 'Importance': feature_importances})

# Sort the DataFrame by importance values
importance_df = importance_df.sort_values(by='Importance', ascending=False)

# Plot the feature importances
plt.figure(figsize=(10, 6))
plt.bar(importance_df['Feature'], importance_df['Importance'])
plt.xlabel('Feature')
plt.ylabel('Importance')
plt.title('XGBoost Feature Importances')
plt.xticks(rotation=45, ha='right')
plt.tight_layout()
plt.show()


y_pred = model1.predict(X_test)
#y_pred=model.predict(dtest_clf)
y_pred_proba = model1.predict_proba(X_test)[:, 1]

#inverse label encoder
y_test_rev= le.inverse_transform(y_test)
y_pred_rev=le.inverse_transform(y_pred)

#print classification report
from sklearn import metrics
from sklearn.metrics import classification_report
accuracy = accuracy_score(y_test, y_pred)

from sklearn.metrics import classification_report

print(classification_report(y_test_rev, y_pred_rev))

#print visualization
plt.figure(figsize=(10, 6))
plt.plot(y_test.values[:300], label='Actual', marker='o')
plt.plot(y_pred[:300], label='Predicted', marker='o')

plt.title('Actual vs. Predicted Values')
plt.xlabel('Sample Index')
plt.ylabel('Values')
plt.legend()
plt.show()



#calculating how often the prediction was correct with a threshold from 100
threshold= 1.0
absolute_difference =np.abs(y_test.values -y_pred)
correct_predictions = np.sum(absolute_difference <= threshold)

accuracy = correct_predictions / len(y_test)

print(f'Correct Predictions: {correct_predictions}/{len(y_test)}')
print(f'Accuracy: {accuracy * 100:.2f}%')

#find out which is the best Feature combination
import numpy as np
from sklearn.metrics import accuracy_score
from xgboost import XGBClassifier

modelxg = XGBClassifier(random_state=42)
modelxg.fit(X_train, y_train)


feature_names = X.columns

# XGBoost Classifier
best_xgboost_model = None
best_xgboost_score = 0.0
best_feature_set_xgboost = None

# Get feature importances
feature_importances = modelxg.feature_importances_

# Sort features by importance
sorted_indices = np.argsort(feature_importances)[::-1]
sorted_feature_names = [feature_names[i] for i in sorted_indices]


for i in range(1, len(sorted_indices) + 1):
    selected_features = sorted_indices[:i]
    X_train_subset = X_train.iloc[:, selected_features]
    X_test_subset = X_test.iloc[:, selected_features]
    selected_feature_names = sorted_feature_names[:i]

    # Train the XGBoost model on the subset of features
    xgboost_model = XGBClassifier(random_state=42)
    xgboost_model.fit(X_train_subset, y_train)

    # Make predictions on the test set
    y_pred_subset_xgboost = xgboost_model.predict(X_test_subset)

    # Evaluate model performance
    score_xgboost = xgboost_model.score(X_test_subset, y_test)

    # Update the best XGBoost model if the current subset performs better
    if score_xgboost > best_xgboost_score:
        best_xgboost_score = score_xgboost
        best_xgboost_model = xgboost_model
        best_feature_set_xgboost = selected_feature_names

# Print the best XGBoost model and its score
print(f"Best XGBoost Model: {best_xgboost_model}")
print(f"Best XGBoost Score: {best_xgboost_score:.4f}")
print(f"Best Feature Set for XGBoost: {best_feature_set_xgboost}")

"""best selected features"""

#use the best selected features from before
X=df[['Net','Speed']]

y=df['MsgRank_encoded']

X_train,X_test,y_train,y_test=train_test_split(X, y,test_size=0.3,shuffle=False,random_state=80)

model1=XGBClassifier(random_state=42)
model1.fit(X_train, y_train)
y_pred = model1.predict(X_test)

y_test_rev= le.inverse_transform(y_test)
y_pred_rev=le.inverse_transform(y_pred)

accuracy = accuracy_score(y_test, y_pred)

from sklearn.metrics import classification_report

print(classification_report(y_test_rev, y_pred_rev))

#visualize the results
plt.figure(figsize=(10, 6))
plt.plot(y_test.values[:100], label='Actual', marker='o')
plt.plot(y_pred[:100], label='Predicted', marker='o')

plt.title('Actual vs. Predicted Values')
plt.xlabel('Sample Index')
plt.ylabel('Values')
plt.legend()
plt.show()

#calculating how often the prediction was correct with a threshold from 100
threshold= 1.0
absolute_difference =np.abs(y_test.values -y_pred)
correct_predictions = np.sum(absolute_difference <= threshold)

accuracy = correct_predictions / len(y_test)

print(f'Correct Predictions: {correct_predictions}/{len(y_test)}')
print(f'Accuracy: {accuracy * 100:.2f}%')