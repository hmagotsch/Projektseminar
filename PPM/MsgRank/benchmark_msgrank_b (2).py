# -*- coding: utf-8 -*-
"""Benchmark MsgRank B.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/138W3rpOBWx6WZJLlymg8-Nna1i6a4IJK
"""

import tensorflow as tf
import os
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import imblearn
import xgboost as xgb
from sklearn.model_selection import train_test_split
from xgboost import plot_importance, plot_tree
from sklearn.metrics import mean_squared_error, mean_absolute_error
import seaborn as sns
from sklearn.preprocessing import LabelEncoder
from xgboost import XGBClassifier
import xgboost as xgb
from sklearn.metrics import mean_squared_error
color_pal = sns.color_palette()
from sklearn.metrics import accuracy_score, classification_report

#path="/content/drive/MyDrive/MachineB_with_Job.csv"
#path="/content/drive/MyDrive/MachineA_with_Job final.csv"
#path="/content/drive/MyDrive/MachineC_with_Job1.csv"
path="/content/drive/MyDrive/MachineD_with_Job1.csv"
df=pd.read_csv(path,sep=";")
df.head(20)
df.info()

#calculating the correlation coefficient for MsgRank and LocID to learn more about there relationship
correlation_coefficient = df['MsgRank'].corr(df['LocID1'])
print(f"Correlation Coefficient: {correlation_coefficient}")

#count how often each MsgRank occurs
value_counts = df['MsgRank'].value_counts()

# display the result
print(value_counts)

df['MsgRank'] = df['MsgRank'].dropna()
# Convert 'MsgRank' column to integers
df

df['MsgRank'].value_counts()

unexpected_values = df['MsgRank'][~df['MsgRank'].apply(lambda x: str(x).isdigit())]
print("Unexpected values:", unexpected_values)

#drop MsgRanks which are smaller than 0
df = df.drop(df.index[df['MsgRank']<0])

#visulaize the results with mathplot lib
msgrank_counts = df['MsgRank'].value_counts()
plt.pie(msgrank_counts, labels=msgrank_counts.index, startangle=90,rotatelabels=True )
plt.title("Maschine B", fontweight='bold')
plt.show()

#visualize the results in plotly
import plotly.graph_objects as go

# Define custom colors for the pie chart
custom_colors = ['#093D79', '#4A688F', '#8BBCE4', '#9D9E9E', '#D1D4D4', '#FFFFFF', '#E40613', '#FA6F7C', '#E4A7AA', '#F39200', '#E1BC89']

# Create a pie chart
fig = go.Figure(data=[go.Pie(labels=msgrank_counts.index, values=msgrank_counts, marker=dict(colors=custom_colors))])

# Update layout
fig.update_layout(title='Maschine D')

# Show plot
fig.show()

#convert CheckIn and CheckOut to datetime
date_format = "%Y-%m-%d %H:%M:%S%z"
df['CheckIn'] = df['CheckIn'].str.replace('\.\d+', '', regex=True)
df['CheckIn'] = pd.to_datetime(df['CheckIn'], format=date_format)

#df['CheckIn']=pd.to_datetime(df['CheckIn'],format='mixed')

#drop MsgRanks which are smaller than 0
df = df.drop(df.index[df['MsgRank']<0])

df.dropna(subset=['MsgRank'],inplace=True)

#use Label Encoder
le = LabelEncoder()
df['MsgRank_encoded']= le.fit_transform(df['MsgRank'])

df['MsgRank_encoded']
df['MsgRank']

#preparation for the Naive Prediction (predict the last value)
df['Last']=df['MsgRank_encoded'].shift(1).fillna(df['MsgRank_encoded'].iloc[0]).astype(int)

#check for null values
df.isnull().any()

bye=['MsgRank']
X=df.drop(bye,axis=1)

y=df['MsgRank_encoded']

X

#perform the train test split
X_train,X_test,y_train,y_test=train_test_split(X, y,test_size=0.3,shuffle=False,random_state=80)

print(len(X_train))
print(len(X_test))

#calculate the Benchmark for MsgRank by always predicting the most frequent value which is encoded the value 5
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, roc_curve

class ConstantClassifier:
  def fit(self,X,y=None):
    pass
  def predict(self, X):
    #return[7]*len(X)
    return[5]*len(X)


model = ConstantClassifier()
model.fit(X_train, y_train)

y_pred = model.predict(X_test)

#reverse the label Encoder
y_test_rev= le.inverse_transform(y_test)
y_pred_rev=le.inverse_transform(y_pred)

#print the classification report
from sklearn import metrics
from sklearn.metrics import classification_report
accuracy = accuracy_score(y_test, y_pred)


from sklearn.metrics import classification_report

print(classification_report(y_test_rev, y_pred_rev))

#visualize the results
plt.figure(figsize=(10, 6))
plt.plot(y_test.values[:300], label='Actual', marker='o')
plt.plot(y_pred[:300], label='Predicted', marker='o')

plt.title('Actual vs. Predicted Values')
plt.xlabel('Sample Index')
plt.ylabel('Values')
plt.legend()
plt.show()

#calculating how often the prediction was correct with a threshold from 100
threshold= 1.0
absolute_difference =np.abs(y_test.values -y_pred)
correct_predictions = np.sum(absolute_difference <= threshold)

accuracy = correct_predictions / len(y_test)

print(f'Correct Predictions: {correct_predictions}/{len(y_test)}')
print(f'Accuracy: {accuracy * 100:.2f}%')

"""Naive Forecast"""

# prediction is the value 'Last' which was assigned earlier in the code
prediction= X_test['Last']

prediction

y_test_rev= le.inverse_transform(y_test)
y_pred_rev=le.inverse_transform(prediction)

results_df = pd.DataFrame({'Actual': y_test, 'Predicted': prediction})
results_df

#print classification report
from sklearn import metrics
from sklearn.metrics import classification_report
accuracy = accuracy_score(y_test, prediction)

from sklearn.metrics import classification_report

print(classification_report(y_test, prediction))

#visualize the results
results_df = results_df.reset_index(drop=True)
plt.figure(figsize=(10, 6))
plt.plot(results_df['Actual'][:100], label='Actual', marker='o')
plt.plot(results_df['Predicted'][:100], label='Predicted', marker='o')


plt.title('Actual vs. Predicted Values')
plt.xlabel('Sample Index')
plt.ylabel('Values')
plt.legend()
plt.show()